import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import sys
from scipy import stats
import pickle
import math
from math import sqrt
import numpy as np
from feature_functions import load_pickle
import os
from sklearn.metrics import r2_score
from torch.autograd import Variable
from tqdm import tqdm
from scipy.ndimage import convolve1d
from scipy.ndimage import gaussian_filter1d
from scipy.signal.windows import triang
import pandas as pd
from collections import Counter
from collections import defaultdict
from scipy.stats import pearsonr
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from scipy.stats import spearmanr

def load_data( path, has_label ):
    '''
    Load features generated by /code/gen_features.py.
    '''
    compounds = np.array( load_pickle( os.path.join(path, 'compounds.pkl') ), dtype=object )
    adjacencies = np.array( load_pickle( os.path.join(path, 'adjacencies.pkl') ), dtype=object )
    fps =  np.array( load_pickle( os.path.join(path, 'fps.pkl') ), dtype=object )
    proteins = np.array( load_pickle( os.path.join(path, 'proteins.pkl') ), dtype=object )
    inv_Temp = np.array( load_pickle( os.path.join(path,  'inv_Temp.pkl' ) ) , dtype=object)
    Temp = np.array( load_pickle( os.path.join(path,  'Temp.pkl' ) ) , dtype=object)
    smiles_name = np.array( load_pickle( os.path.join(path,  'smiles_names.pkl' ) ) , dtype=object)
    seq_name = np.array( load_pickle( os.path.join(path,  'seq_names.pkl' ) ) , dtype=object)
    if has_label:
        targets = np.array( load_pickle( os.path.join(path,  'log10_kcat.pkl' ) ) , dtype=object)
        data_pack = [ compounds, adjacencies, fps , proteins, inv_Temp, Temp, targets ,smiles_name,seq_name]
    else:
        data_pack = [ compounds, adjacencies, fps , proteins, inv_Temp, Temp ,smiles_name,seq_name]
        
    
    return data_pack

def split_data( data, ratio=0.1):
    '''
    Randomly split data into two datasets.
    '''
    idx = np.arange(len( data[0]))
    np.random.shuffle(idx)
    num_split = int(len(data[0]) * ratio)
    idx_1, idx_0 = idx[:num_split], idx[num_split:]
    data_0 = [ data[di][idx_0] for di in range(len(data))]
    data_1 = [ data[di][idx_1] for di in range(len(data))]
    return data_0, data_1


def get_bin_idx(x):
    return min(int(x // 1), 99)
def get_lds_kernel_window(kernel, ks, sigma):
    assert kernel in ['gaussian', 'triang', 'laplace']
    half_ks = (ks - 1) // 2
    if kernel == 'gaussian':
        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks
        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))
    elif kernel == 'triang':
        kernel_window = triang(ks)
    else:
        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)
        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))

    return kernel_window
def calculate_empirical_label_distribution(bin_index_per_label):
    num_samples_of_bins = dict(Counter(bin_index_per_label))
    Nb = max(bin_index_per_label) + 1
    emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]
    return emp_label_dist

def calculate_effective_label_distribution(emp_label_dist, kernel='gaussian', ks=5, sigma=2):
    lds_kernel_window = get_lds_kernel_window(kernel=kernel, ks=ks, sigma=sigma)
    eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')
    return eff_label_dist

def batch_pad(arr):
    '''
    Pad feature vectors all into the same length.
    '''
    N = max([a.shape[0] for a in arr])
    if arr[0].ndim == 1:
        new_arr = np.zeros((len(arr), N))
        new_arr_mask = np.zeros((len(arr), N))
        for i, a in enumerate(arr):
            n = a.shape[0]
            new_arr[i, :n] = a + 1
            new_arr_mask[i, :n] = 1
        return new_arr, new_arr_mask

    elif arr[0].ndim == 2:
        new_arr = np.zeros((len(arr), N, N))
        new_arr_mask = np.zeros((len(arr), N, N))
        for i, a in enumerate(arr):
            n = a.shape[0]
            new_arr[i, :n, :n] = a
            new_arr_mask[i, :n, :n] = 1
        return new_arr, new_arr_mask
    
def batch2tensor(batch_data, has_label, device):
    '''
    Convert loaded data into torch tensors.
    '''
    # 同时用了min-size个数量的data
    atoms_pad, atoms_mask = batch_pad(batch_data[0])
    adjacencies_pad, _ = batch_pad(batch_data[1])
    
    fps = batch_data[2]
    temp_arr = np.zeros((len(fps), 1024))
    for i,a in enumerate(fps):
        temp_arr[i, :] = np.array(list(a), dtype=int)
    fps = temp_arr
    
    amino_pad, amino_mask = batch_pad(batch_data[3])
    
    atoms_pad = Variable(torch.LongTensor(atoms_pad)).to(device)
    atoms_mask = Variable(torch.FloatTensor(atoms_mask)).to(device)
    adjacencies_pad = Variable(torch.LongTensor(adjacencies_pad)).to(device)
    fps = Variable(torch.FloatTensor(fps)).to(device)
    amino_pad = Variable(torch.LongTensor(amino_pad)).to(device)
    amino_mask = Variable(torch.FloatTensor(amino_mask)).to(device)

    inv_Temp = batch_data[4]
    temp_arr = np.zeros((len(inv_Temp), 1))
    for i,a in enumerate( inv_Temp ):
        temp_arr[i, :] = a
    inv_Temp = torch.FloatTensor(temp_arr).to(device)
    
    Temp = batch_data[5]
    temp_arr = np.zeros((len(Temp), 1))
    for i,a in enumerate( Temp ):
        temp_arr[i, :] = a
    Temp = torch.FloatTensor(temp_arr).to(device)
    
    smiles_names = batch_data[-2]
    seq_names = batch_data[-1]
    if has_label == False:
        return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, inv_Temp, Temp ,smiles_names,seq_names
    else:
        label = batch_data[6]
        temp_arr = np.zeros((len(label), 1))
        for i,a in enumerate( label ):
            temp_arr[i, :] = a
        label = torch.FloatTensor(temp_arr).to(device)

    

    return atoms_pad, atoms_mask, adjacencies_pad, fps, amino_pad, amino_mask, inv_Temp, Temp, label, smiles_names,seq_names 
def get_domain(label):
    return int(np.floor(label)+8) 

def get_prototypes(features, labels):
    domain_features = defaultdict(list)

    for feature, label in zip(features, labels):
        domain = get_domain(label)  
        domain_features[domain].append(feature.detach().cpu().numpy())  
    domain_means = [np.mean(np.vstack(domain_features[domain]), axis=0) for domain in domain_features]

    # 返回 15 * 122 的矩阵
    return np.vstack(domain_means)
def compute_cosine_similarity(cat_vector, domain_prototypes,device):
    # cat_vector = torch.tensor(cat_vector).to(device)
    cat_vector = cat_vector.clone().detach().to(device)  
    domain_prototypes = torch.tensor(domain_prototypes).to(device)

    cosine_sim = F.cosine_similarity(cat_vector.unsqueeze(1), domain_prototypes.unsqueeze(0), dim=2)  # (batch_size, num_domains)

    return cosine_sim

def apply_softmax(cosine_sim):
    return F.softmax(cosine_sim, dim=1)  
def scores(label, pred ):
    '''
    Compute R2 and RMSE scores of predicted values.
    '''
    label = label.reshape(-1)
    pred = pred.reshape(-1)
    rmse = sqrt(((label - pred)**2).mean(axis=0))
    r2 = r2_score( label , pred  )
    return round(rmse, 6), round(r2,6)

# def randomforest_eval(model, data_pred, device, lr, batch_size, lr_decay, decay_interval, num_epochs,domain_prototypes ):
#     criterion = F.mse_loss
#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)
#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)
#     idx = np.arange(len(data_pred[0]))
    
#     min_size = 4
#     if batch_size > min_size:
#         div_min = int(batch_size / min_size)
    
#     for epoch in range(num_epochs):
             
#         np.random.shuffle(idx)
#         model.train()
#         predictions = []
#         labels = []
#         cat_vector_list = []

#         for i in tqdm(range(math.ceil( len(data_pred[0]) / min_size )),desc='Train:'):
#             batch_data = [data_pred[di][idx[ i* min_size: (i + 1) * min_size]] \
#                           for di in range(len(data_pred))]

#             atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\
#                             amino_mask, inv_Temp, Temp, label,smiles_names,seq_names = batch2tensor(batch_data, True, device)
                        
#             pred,cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp ,smiles_names,seq_names,epoch,label)
            
#             cat_vector_list.append(cat_vector)
#             cosine_sim = compute_cosine_similarity(cat_vector, domain_prototypes,device)
#             # softmax_sim = apply_softmax(cosine_sim)
#             domain_labels = [get_domain(x.cpu()) for x in label]
#             domain_labels = torch.tensor(domain_labels, dtype=torch.long).to(device)
#             lce_loss = F.cross_entropy(cosine_sim, domain_labels)
#             loss_weight = 0.5
#             loss = loss_weight * criterion(pred.float(), label.float()) + (1 - loss_weight) * lce_loss

#             predictions += pred.cpu().detach().numpy().reshape(-1).tolist()
#             labels += label.cpu().numpy().reshape(-1).tolist()
#             loss.backward()

#             if i % div_min == 0 and i != 0:    
#                 optimizer.step()
#                 optimizer.zero_grad()

#         scheduler.step()
        
#     return cat_vector_list 
# def finetune_eval(model, data_train, data_test, data_dev, device, lr, batch_size, lr_decay, decay_interval, num_epochs,domain_prototypes ):
#     criterion = F.mse_loss
#     optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)
#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)
#     idx = np.arange(len(data_train[0]))
    
#     min_size = 4
#     if batch_size > min_size:
#         div_min = int(batch_size / min_size)
    
#     label_list = data_train[6]
#     label_list = np.ravel(label_list)
#     label_list = np.array(label_list, dtype=np.float32)
        
#     rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores = [],[],[],[],[],[]

#     for epoch in range(num_epochs):
             
#         np.random.shuffle(idx)
#         model.train()
#         predictions = []
#         labels = []
#         cat_vector_list = []

#         for i in tqdm(range(math.ceil( len(data_train[0]) / min_size )),desc='Train:'):
#             batch_data = [data_train[di][idx[ i* min_size: (i + 1) * min_size]] \
#                           for di in range(len(data_train))]

#             atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\
#                             amino_mask, inv_Temp, Temp, label,smiles_names,seq_names = batch2tensor(batch_data, True, device)
                        
#             pred,cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp ,smiles_names,seq_names,epoch,label)
            
#             cat_vector_list.append(cat_vector)
#             cosine_sim = compute_cosine_similarity(cat_vector, domain_prototypes,device)
#             # softmax_sim = apply_softmax(cosine_sim)
#             domain_labels = [get_domain(x.cpu()) for x in label]
#             domain_labels = torch.tensor(domain_labels, dtype=torch.long).to(device)
#             lce_loss = F.cross_entropy(cosine_sim, domain_labels)
#             loss_weight = 0.5
#             loss = loss_weight * criterion(pred.float(), label.float()) + (1 - loss_weight) * lce_loss

#             predictions += pred.cpu().detach().numpy().reshape(-1).tolist()
#             labels += label.cpu().numpy().reshape(-1).tolist()
#             loss.backward()

#             if i % div_min == 0 and i != 0:    
#                 optimizer.step()
#                 optimizer.zero_grad()
        
#         # cat_vector_tensor = torch.cat(cat_vector_list, dim=0) 
#         # label_tensor = torch.from_numpy(label_list)

#         # if epoch >= 1:
#         #     model.Fds.update_last_epoch_stats(epoch)
#         #     model.Fds.update_running_stats(cat_vector_tensor, label_tensor, epoch)
        
#         predictions = np.array(predictions)
#         labels = np.array(labels)
#         rmse_train, r2_train = scores( labels, predictions )
#         rmse_dev, r2_dev = test( model,  data_dev, batch_size, device ) #dev dataset
#         rmse_test, r2_test = test(model, data_test, batch_size, device) # test dataset

#         if r2_test > 0.3:
#             print('Best model found at epoch=' + str(epoch) + '!')
#             best_model_pth = '../data/model_finetune/model_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)
#             best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'_r2test='+str(round(r2_test,4))+'.pth'
#             torch.save( model.state_dict(), best_model_pth)

#         rmse_train_scores.append( rmse_train )
#         r2_train_scores.append( r2_train )
#         rmse_dev_scores.append( rmse_dev )
#         r2_dev_scores.append( r2_dev )
#         rmse_test_scores.append( rmse_test )
#         r2_test_scores.append( r2_test )

#         print('epoch: '+str(epoch)+'/'+ str(num_epochs) +';  rmse test: ' + str(rmse_test) + '; r2 test: ' + str(r2_test) )
#         scheduler.step()
        
#     return rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores 
def train_eval(model, data_train, data_test, data_dev, device, lr, batch_size, lr_decay, decay_interval, num_epochs ):
    criterion = F.mse_loss
    
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0, amsgrad=True)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size= decay_interval, gamma=lr_decay)
    idx = np.arange(len(data_train[0]))
    
    min_size = 4
    if batch_size > min_size:
        div_min = int(batch_size / min_size)
    
    label_list = data_train[6]
    label_list = np.ravel(label_list)
    label_list = np.array(label_list, dtype=np.float32)
        
    rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores = [],[],[],[],[],[]
    pcc_test_scores = []
    mae_test_scores = []
    proto_epoch = 1

    for epoch in range(num_epochs):
             
        np.random.shuffle(idx)
        model.train()
        predictions = []
        labels = []
        cat_vector_list = []
        cat_vector_proto_list = []

        for i in tqdm(range(math.ceil( len(data_train[0]) / min_size )),desc='Train:'):
            batch_data = [data_train[di][idx[ i* min_size: (i + 1) * min_size]] \
                          for di in range(len(data_train))]

            atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\
                            amino_mask, inv_Temp, Temp, label,smiles_names,seq_names = batch2tensor(batch_data, True, device)
                        
            pred,cat_vector = model( atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp ,smiles_names,seq_names,epoch,label)
            
            cat_vector_list.append(cat_vector)

            if epoch > proto_epoch :
                cosine_sim = compute_cosine_similarity(cat_vector, domain_prototypes,device)
                # cosine_sim = apply_softmax(cosine_sim)
                domain_labels = [get_domain(x.cpu()) for x in label]
                domain_labels = torch.tensor(domain_labels, dtype=torch.long).to(device)
                lce_loss = F.cross_entropy(cosine_sim, domain_labels)
                loss_weight = 0.5
                loss = loss_weight * criterion(pred.float(), label.float()) + (1 - loss_weight) * lce_loss
            else :
                loss = criterion(pred.float(), label.float())

            # loss = criterion(pred.float(), label.float())

            predictions += pred.cpu().detach().numpy().reshape(-1).tolist()
            labels += label.cpu().numpy().reshape(-1).tolist()
            loss.backward()

            if i % div_min == 0 and i != 0:    
                optimizer.step()
                optimizer.zero_grad()

        for vector in cat_vector_list:
            for i in range(vector.shape[0]):
                cat_vector_proto_list.append(vector[i:i+1])
        
        cat_vector_tensor = torch.cat(cat_vector_list, dim=0) 
        label_tensor = torch.from_numpy(label_list)

        if epoch >= 1:
            model.Fds.update_last_epoch_stats(epoch)
            model.Fds.update_running_stats(cat_vector_tensor, label_tensor, epoch)
        
        predictions = np.array(predictions)
        labels = np.array(labels)
        rmse_train, r2_train = scores( labels, predictions )
        # rmse_dev, r2_dev = test( model,  data_dev, batch_size, device ) #dev dataset
        # rmse_test, r2_test = test(model, data_test, batch_size, device) # test dataset
        rmse_dev, r2_dev, pcc_dev, mae_dev,scc_dev = test( model,  data_dev, batch_size, device ) #dev dataset
        rmse_test, r2_test, pcc_test, mae_test,scc_test = test(model, data_test, batch_size, device) # test dataset

        pcc_test_scores.append(pcc_test)
        mae_test_scores.append(mae_test)
        
        # print(pcc_test_scores,mae_test_scores)

        rmse_train_scores.append( rmse_train )
        r2_train_scores.append( r2_train )
        rmse_dev_scores.append( rmse_dev )
        r2_dev_scores.append( r2_dev )
        rmse_test_scores.append( rmse_test )
        r2_test_scores.append( r2_test )

        print('epoch: '+str(epoch)+'/'+ str(num_epochs) +';  rmse test: ' + str(rmse_test) + '; r2 test: ' + str(r2_test) +';  pcc test: ' + str(pcc_test) + '; mae test: ' + str(mae_test)+ '; scc test: ' + str(scc_test))
        
        scheduler.step()

        if epoch == proto_epoch:
            domain_prototypes_origin = get_prototypes(cat_vector_proto_list, labels)
            domain_prototypes = domain_prototypes_origin
        elif epoch > proto_epoch :
            domain_prototypes_update = get_prototypes(cat_vector_proto_list, labels)
            domain_prototypes = 0.9 * domain_prototypes + 0.1 * domain_prototypes_update

        if r2_test > 0.7:
            print('Best model found at epoch=' + str(epoch) + '!')
            best_model_pth = '../data/model_mpek/model/model_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)
            best_model_pth = best_model_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'_r2test='+str(round(r2_test,4))+'.pth'
            torch.save( model.state_dict(), best_model_pth)

            best_model_prototypes_pth = '../data/model_mpek/proto/prototypes_latentdim=' + str(model.latent_dim) + '_outlayer=' + str(model.layer_out)
            best_model_prototypes_pth = best_model_prototypes_pth + '_rmsetest='+str( round(rmse_test,4) )+'_rmsedev='+str( round(rmse_dev,4) ) +'_r2test='+str(round(r2_test,4))+'.pkl'
            torch.save(domain_prototypes, best_model_prototypes_pth)
        
    return rmse_train_scores, r2_train_scores, rmse_test_scores, r2_test_scores, rmse_dev_scores, r2_dev_scores 
   
def test(model, data_test, batch_size, device):
    model.eval()
    predictions = []
    labels = []
    
    for i in range(math.ceil(len(data_test[0]) / batch_size)):
        batch_data = [data_test[di][i * batch_size: (i + 1) * batch_size] for di in range(len(data_test))]
        atoms_pad, atoms_mask, adjacencies_pad, batch_fps, amino_pad,\
                    amino_mask, inv_Temp, Temp, label, smiles_names, seq_names = batch2tensor(batch_data, True, device)
        
        with torch.no_grad():
            pred, cat_vector = model(atoms_pad, atoms_mask, adjacencies_pad, amino_pad, amino_mask, batch_fps, inv_Temp, Temp, smiles_names, seq_names, epoch=0, label=None)
            
        predictions += pred.cpu().detach().numpy().reshape(-1).tolist()
        labels += label.cpu().numpy().reshape(-1).tolist()
  
    
    predictions = np.array(predictions)
    labels = np.array(labels)
    
    # 计算RMSE和R2
    rmse, r2 = scores(labels, predictions)
    
    # 计算PCC
    pcc, _ = pearsonr(labels, predictions)
    
    # 计算MAE
    mae = mean_absolute_error(labels, predictions)

    scc, _ = spearmanr(labels, predictions)
    
    return rmse, r2, pcc, mae, scc
